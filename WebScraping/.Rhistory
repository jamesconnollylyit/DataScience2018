library(tidyverse)
install.packages("tidyverse")
library(tidyverse)
install.packages("rvest")
library(rvest)
install.packages("stringr")
library(stringr)
url <- 'http://www.trustpilot.com/review/www.amazon.com'
    # The '.' indicates the class     html_nodes('.pagination-page') %>%     # Extract the raw text as a list     html_text()     # The second to last of the buttons is the one     pages_data[(length(pages_data) - 1)] %>%     # Take the raw string     unname() %>%     # Convert to number     as.numeric() }
library(stringr)
library(rebus)
install.packages("rebus")
library(rebus)
get_last_page <- function(html) {     pages_data <- html %>%     # The '.' indicates the class     html_nodes('.pagination-page') %>%     # Extract the raw text as a list     html_text()     # The second to last of the buttons is the one     pages_data[(length(pages_data) - 1)] %>%     # Take the raw string     unname() %>%     # Convert to number     as.numeric() }
first_page <- read_html(url)
(latest_page_number <- get_last_page(first_page))
list_of_pages <- str_c(url, '?page=', 1:latest_page_number)
list_of_pages
get_reviews <- function(html) {     html %>%     # The relevant tag     html_nodes('.review-body') %>%         html_text() %>%     # Trim additional white space     str_trim() %>%     # Convert the list into a vector     unlist() }
get_reviewer_names <- function(html) {     html %>%         html_nodes('.user-review-name-link') %>%         html_text() %>%         str_trim() %>%         unlist() }
get_review_dates <- function(html) {     status <- html %>%                   html_nodes('time') %>%     # The status information is this time a tag attribute     html_attrs() %>%     # Extract the second element     map(2) %>%                   unlist()     dates <- html %>%                   html_nodes('time') %>%                   html_attrs() %>%                   map(1) %>%     # Parse the string into a datetime object with lubridate     ymd_hms() %>%                   unlist()     # Combine the status and the date information to filter one via the other     return_dates <- tibble(status = status, dates = dates) %>%     # Only these are actual reviews     filter(status == 'ndate') %>%     # Select and convert to vector     pull(dates) %>%     # Convert DateTimes to POSIX objects     as.POSIXct(origin = '1970-01-01 00:00:00')     # The lengths still occasionally do not lign up. You then arbitrarily crop the dates to fit     # This can cause data imperfections, however reviews on one page are generally close in time)     length_reviews <- length(get_reviews(html))     return_reviews <- if (length(return_dates) > length_reviews) {         return_dates[1:length_reviews]     } else {         return_dates     }     return_reviews }
get_review_dates <- function(html) {     status <- html %>%                   html_nodes('time') %>%     # The status information is this time a tag attribute     html_attrs() %>%     # Extract the second element     map(2) %>%                   unlist()     dates <- html %>%                   html_nodes('time') %>%                   html_attrs() %>%                   map(1) %>%     # Parse the string into a datetime object with lubridate     ymd_hms() %>%                   unlist()     # Combine the status and the date information to filter one via the other     return_dates <- tibble(status = status, dates = dates) %>%     # Only these are actual reviews     filter(status == 'ndate') %>%     # Select and convert to vector     pull(dates) %>%     # Convert DateTimes to POSIX objects     as.POSIXct(origin = '1970-01-01 00:00:00')     # The lengths still occasionally do not lign up. You then arbitrarily crop the dates to fit     # This can cause data imperfections, however reviews on one page are generally close in time)     length_reviews <- length(get_reviews(html))     return_reviews <- if (length(return_dates) > length_reviews) {         return_dates[1:length_reviews]     } else {         return_dates     }     return_reviews }
get_star_rating <- function(html) {     # The pattern you look for: the first digit after `count-`     pattern = 'count-' %R% capture(DIGIT)     ratings <- html %>%         html_nodes('.star-rating') %>%         html_attrs() %>%     # Apply the pattern match to all attributes     map(str_match, pattern = pattern) %>%     # str_match[1] is the fully matched string, the second entry     # is the part you extract with the capture in your pattern       map(2) %>%         unlist()     # Leave out the first instance, as it is not part of a review     ratings[2:length(ratings)] }
get_data_table <- function(html, company_name) {     # Extract the Basic information from the HTML     reviews <- get_reviews(html)     reviewer_names <- get_reviewer_names(html)     dates <- get_review_dates(html)     ratings <- get_star_rating(html)     # Combine into a tibble     combined_data <- tibble(reviewer = reviewer_names,                               date = dates,                               rating = ratings,                               review = reviews)     # Tag the individual data with the company name     combined_data %>%         mutate(company = company_name) %>%         select(company, reviewer, date, rating, review) }
get_data_from_url <- function(url, company_name) {     html <- read_html(url)     get_data_table(html, company_name) }
scrape_write_table <- function(url, company_name) {     # Read first page     first_page <- read_html(url)     # Extract the number of pages that have to be queried     latest_page_number <- get_last_page(first_page)     # Generate the target URLs     list_of_pages <- str_c(url, '?page=', 1:latest_page_number)     # Apply the extraction and bind the individual results back into one table,      # which is then written as a tsv file into the working directory     list_of_pages %>%     # Apply to all URLs     map(get_data_from_url, company_name) %>%     # Combine the tibbles into one tibble     bind_rows() %>%     # Write a tab-separated file     write_tsv(str_c(company_name, '.tsv')) }
scrape_write_table(url, 'amazon') amz_tbl <- read_tsv('amazon.tsv') tail(amz_tbl, 5)
scrape_write_table(url, 'amazon')
    first_page <- read_html(url)
    latest_page_number <- get_last_page(first_page)
    list_of_pages <- str_c(url, '?page=', 1:latest_page_number)
    list_of_pages %>%     map(get_data_from_url, company_name) %>%     bind_rows() %>%     write_tsv(str_c(company_name, '.tsv'))
scrape_write_table(url, 'amazon')
rtvs::debug_source("C:/Users/James/Source/Repos/DataScience2018/WebScraping/script.R")
rtvs::debug_source("C:/Users/James/Source/Repos/DataScience2018/WebScraping/script.R")
n
s
s
s
s
s
s
n
n
Q
rtvs::debug_source("C:/Users/James/Source/Repos/DataScience2018/WebScraping/script.R")
n
Q
rtvs::debug_source("C:/Users/James/Source/Repos/DataScience2018/WebScraping/script.R")
n
s
c
url
scrape_write_table(url, 'amazon')
scrape_write_table(url, 'amazon')
scrape_write_table <- function(url, company_name) {     # Read first page     first_page <- read_html(url)     # Extract the number of pages that have to be queried     latest_page_number <- get_last_page(first_page)     # Generate the target URLs     list_of_pages <- str_c(url, '?page=', 1:latest_page_number)     # Apply the extraction and bind the individual results back into one table,      # which is then written as a tsv file into the working directory     list_of_pages %>%     # Apply to all URLs     map(get_data_from_url, company_name) %>%     # Combine the tibbles into one tibble     bind_rows() %>%     # Write a tab-separated file     write_tsv(str_c(company_name, '.tsv')) }
url
scrape_write_table(url, 'amazon')
scrape_write_table(url, 'amazon')
get_reviewer_names <- function(html) {     html %>%         html_nodes('.consumer-info__details__name') %>%         html_text() %>%         str_trim() %>%         unlist() }
scrape_write_table(url, 'amazon')
url <- 'http://www.trustpilot.com/review/www.amazon.com' get_last_page <- function(html) {     pages_data <- html %>%     # The '.' indicates the class     html_nodes('.pagination-page') %>%     # Extract the raw text as a list     html_text()     # The second to last of the buttons is the one     pages_data[(length(pages_data) - 1)] %>%     # Take the raw string     unname() %>%     # Convert to number     as.numeric() } first_page <- read_html(url) (latest_page_number <- get_last_page(first_page)) list_of_pages <- str_c(url, '?page=', 1:latest_page_number) list_of_pages get_reviews <- function(html) {     html %>%     # The relevant tag     html_nodes('.review-body') %>%         html_text() %>%     # Trim additional white space     str_trim() %>%     # Convert the list into a vector     unlist() } get_reviewer_names <- function(html) {     html %>%         html_nodes('.consumer-info__details__name') %>%         html_text() %>%         str_trim() %>%         unlist() } get_review_dates <- function(html) {     status <- html %>%                   html_nodes('time') %>%     # The status information is this time a tag attribute     html_attrs() %>%     # Extract the second element     map(2) %>%                   unlist()     dates <- html %>%                   html_nodes('time') %>%                   html_attrs() %>%                   map(1) %>%     # Parse the string into a datetime object with lubridate     ymd_hms() %>%                   unlist()     # Combine the status and the date information to filter one via the other     return_dates <- tibble(status = status, dates = dates) %>%     # Only these are actual reviews     filter(status == 'ndate') %>%     # Select and convert to vector     pull(dates) %>%     # Convert DateTimes to POSIX objects     as.POSIXct(origin = '1970-01-01 00:00:00')     # The lengths still occasionally do not lign up. You then arbitrarily crop the dates to fit     # This can cause data imperfections, however reviews on one page are generally close in time)     length_reviews <- length(get_reviews(html))     return_reviews <- if (length(return_dates) > length_reviews) {         return_dates[1:length_reviews]     } else {         return_dates     }     return_reviews } get_review_dates <- function(html) {     status <- html %>%                   html_nodes('time') %>%     # The status information is this time a tag attribute     html_attrs() %>%     # Extract the second element     map(2) %>%                   unlist()     dates <- html %>%                   html_nodes('time') %>%                   html_attrs() %>%                   map(1) %>%     # Parse the string into a datetime object with lubridate     ymd_hms() %>%                   unlist()     # Combine the status and the date information to filter one via the other     return_dates <- tibble(status = status, dates = dates) %>%     # Only these are actual reviews     filter(status == 'ndate') %>%     # Select and convert to vector     pull(dates) %>%     # Convert DateTimes to POSIX objects     as.POSIXct(origin = '1970-01-01 00:00:00')     # The lengths still occasionally do not lign up. You then arbitrarily crop the dates to fit     # This can cause data imperfections, however reviews on one page are generally close in time)     length_reviews <- length(get_reviews(html))     return_reviews <- if (length(return_dates) > length_reviews) {         return_dates[1:length_reviews]     } else {         return_dates     }     return_reviews } get_star_rating <- function(html) {     # The pattern you look for: the first digit after `count-`     pattern = 'count-' %R% capture(DIGIT)     ratings <- html %>%         html_nodes('.star-rating') %>%         html_attrs() %>%     # Apply the pattern match to all attributes     map(str_match, pattern = pattern) %>%     # str_match[1] is the fully matched string, the second entry     # is the part you extract with the capture in your pattern       map(2) %>%         unlist()     # Leave out the first instance, as it is not part of a review     ratings[2:length(ratings)] } get_data_table <- function(html, company_name) {     # Extract the Basic information from the HTML     reviews <- get_reviews(html)     reviewer_names <- get_reviewer_names(html)     dates <- get_review_dates(html)     ratings <- get_star_rating(html)     # Combine into a tibble     combined_data <- tibble(reviewer = reviewer_names,                               date = dates,                               rating = ratings,                               review = reviews)     # Tag the individual data with the company name     combined_data %>%         mutate(company = company_name) %>%         select(company, reviewer, date, rating, review) } get_data_from_url <- function(url, company_name) {     html <- read_html(url)     get_data_table(html, company_name) } scrape_write_table <- function(url, company_name) {     # Read first page     first_page <- read_html(url)     # Extract the number of pages that have to be queried     latest_page_number <- get_last_page(first_page)     # Generate the target URLs     list_of_pages <- str_c(url, '?page=', 1:latest_page_number)     # Apply the extraction and bind the individual results back into one table,      # which is then written as a tsv file into the working directory     list_of_pages %>%     # Apply to all URLs     map(get_data_from_url, company_name) %>%     # Combine the tibbles into one tibble     bind_rows() %>%     # Write a tab-separated file     write_tsv(str_c(company_name, '.tsv')) }
scrape_write_table(url, 'amazon')
install.packages("lubridate")
library(lubridate)
scrape_write_table(url, 'amazon')
first_page <- read_html(url)
(latest_page_number <- get_last_page(first_page))
list_of_pages <- str_c(url, '?page=', 1:latest_page_number)
list_of_pages
get_reviews <- function(html) {     html %>%     # The relevant tag     html_nodes('.review-info__body__text') %>%         html_text() %>%     # Trim additional white space     str_trim() %>%     # Convert the list into a vector     unlist() }
scrape_write_table(url, 'amazon')
library(rvest)
f1 <- read_html("http://www.bbc.com/sport/formula1")
class (f1)
f1
f1 <- read_html("http://www.bbc.com/sport/formula1/43702834")
class (f1)
f1
f1 %>% html_nodes('ul') %>% html_nodes('span')
f1
f1 %>% html_nodes('ul') %>% html_nodes('span')
f1 %>% html_nodes('comments')
f1 %>% html_nodes('Comments')
f1 %>% html_nodes('.Comments')
f1 %>% html_nodes('.Comments')
f1 %>% html_nodes('.cmt-title')
f1 %>% html_nodes('ul') %>% html_nodes('span')
f1 %>% html_nodes('.cmt-title')
f1 %>% html_nodes('.cmts-list comments-filter-none')
f1 %>% html_nodes('.cmts-list')
f1 %>% html_nodes('cmts-list')
f1 <- read_html("http://www.bbc.com/sport/formula1/43702834")
class (f1)
f1
f1 %>% html_nodes('ul') %>% html_nodes('span')
f1 %>% html_nodes('.cmts-list')
library(rvest)
url <- 'http://www.imdb.com/search/title?count=100&release_date=2017,2017&title_type=feature'
webpage <- read_html(url)
rank_data_html <- html_nodes(webpage, '.text-primary')
rank_data <- html_text(rank_data_html)
head(rank_data)
rank_data <- as.numeric(rank_data)
head(rank_data)
title_data_html <- html_nodes(webpage, '.lister-item-header a')
title_data <- html_text(title_data_html)
head(title_data)
title_data_html <- html_nodes(webpage, '. a')
library(rvest)
url <- 'http://www.imdb.com/search/title?count=100&release_date=2017,2017&title_type=feature'
webpage <- read_html(url)
rank_data_html <- html_nodes(webpage, '.text-primary')
rank_data <- html_text(rank_data_html)
head(rank_data)
rank_data <- as.numeric(rank_data)
head(rank_data)
title_data_html <- html_nodes(webpage, '.lister-item-header a')
title_data <- html_text(title_data_html)
head(title_data)
description_data_html <- html_nodes(webpage, '.ratings-bar+ .text-muted')
description_data_html <- html_nodes(webpage, '.ratings-metascore .text-muted')
description_data <- html_text(description_data_html)
head(description_data)
description_data_html <- html_nodes(webpage, '.ratings-metascore, strong .text-muted')
description_data <- html_text(description_data_html)
head(description_data)
description_data_html <- html_nodes(webpage,'.ratings-bar+ .text-muted')
#Converting the description data to text
description_data <- html_text(description_data_html)
#Let's have a look at the description data
head(description_data)
description_data_html <- html_nodes(webpage,'.text-muted selectorgadget_selected')
description_data <- html_text(description_data_html)
head(description_data)
description_data_html <- html_nodes(webpage,'.ratings-bar+ .text-muted')
description_data <- html_text(description_data_html)
head(description_data)
description_data_html <- html_nodes(webpage,'.text-muted')
description_data <- html_text(description_data_html)
head(description_data)
library(swirl) install_course_zip("c:/swirl_courses-master.zip", multi = TRUE, which_course = "R Programming") swirl()
library(swirl) install_course_zip("c:/swirl_courses-master.zip", multi = TRUE, which_course = "R Programming") swirl()
ls()
main()
0
main()
library(swirl) install_course_zip("c:/swirl_courses-master.zip", multi = TRUE, which_course = "R Programming") swirl()
library(swirl) install_course_zip("c:/swirl_courses-master.zip", multi = TRUE, which_course = "R Programming") swirl()
ls()
main()
library(swirl) install_course_zip("c:/swirl_courses-master.zip", multi = TRUE, which_course = "Manipulating data with dplyr") swirl()
library(swirl) install_course_zip("c:/swirl_courses-master.zip", multi = TRUE, which_course = "Manipulating_data_with_dplyr") swirl()
swirl()
swirl()
library(swirl) install_course_zip("c:/swirl_courses-master.zip", multi = TRUE, which_course = “Getting_and_cleaning_data") swirl()
library(swirl) install_course_zip("c:/swirl_courses-master.zip", multi = TRUE, which_course = "Getting_and_cleaning_data") swirl()
library(swirl) install_course_zip("c:/swirl_courses-master.zip", multi = TRUE, which_course = "Getting_and_cleaning_data") swirl()
library(swirl) install_course_zip("c:/swirl_courses-master.zip", multi = TRUE, which_course = "Getting and cleaning data") swirl()
library(swirl) install_course_zip("c:/swirl_courses-master.zip", multi = TRUE, which_course = "Getting and cleaning data") swirl(     )
swirl()
library(swirl) install_course_zip("c:/swirl_courses-master.zip", multi = TRUE, which_course = "Getting and cleaning data") swirl()
library(swirl) uninstall_course("swirldev-swirl courses-df512b2") install_from_swirl("R Programming")
library(swirl) install_from_swirl("Getting and cleaning data")
library(swirl) setwd('C:/') install_course_directory('swirl_courses/Getting_and_cleaning_data')+ install_course_directory('swirl_courses/Getting_and_cleaning_data')
library(swirl) setwd('C:/') install_course_directory('swirl_courses/Getting_and_cleaning_data')
library(swirl) Install_from_swirl("Getting and cleaning data") swirl()
library(swirl) Install_from_swirl(“Getting and cleaning data ”) swirl()
library(swirl) Install_from_swirl("R Getting and cleaning data") swirl()
library(swirl)
Install_from_swirl("R Getting and cleaning data")
library(swirl) setwd('d:/swirl') install_course_directory('swirl_courses-master/Getting and cleaning data')
description_data_html <- html_nodes(webpage,'.ratings-bar+ .text-muted')
url <- 'http://www.imdb.com/search/title?count=100&release_date=2017,2017&title_type=feature'
#Reading the HTML code from the website
webpage <- read_html(url)
#Using CSS selectors to scrap the rankings section
rank_data_html <- html_nodes(webpage, '.text-primary')
#Converting the ranking data to text
rank_data <- html_text(rank_data_html)
#Let's have a look at the rankings
head(rank_data)
#Data-Preprocessing: Converting rankings to numerical
rank_data <- as.numeric(rank_data)
#Let's have another look at the rankings
head(rank_data)
rank_data_html <- html_nodes(web_page, '.text-primary')
library(rvest)
url <- 'http://www.imdb.com/search/title?count=100&release_date=2017,2017&title_type=feature'
webpage <- read_html(url)
rank_data_html <- html_nodes(web_page, '.text-primary')
web_page <- read_html(url)
rank_data_html <- html_nodes(web_page, '.text-primary')
rank_data <- html_text(rank_data_html)
head(rank_data)
rank_data <- as.numeric(rank_data)
head(rank_data)
title_data_html <- html_nodes(webpage, '.lister-item-header a')
title_data <- html_text(title_data_html)
head(title_data)
description_data_html <- html_nodes(webpage,'.ratings-bar+ .text-muted')
description_data <- html_text(description_data_html)
head(description_data)
library(swirl)
swirl()
swirl::install_course()
swirl()
mydf <- read.csv(path2csv, stringAsFactors = FALSE)
mydf <- read.csv(path2csv, stringsAsFactors = FALSE)
dim(mydf)
library(rvest)
#Specifying the url for desired website to be scrapped
url <- 'http://www.imdb.com/search/title?count=100&release_date=2017,2017&title_type=feature'
#Reading the HTML code from the website
web_page <- read_html(url)
#Using CSS selectors to scrap the rankings section
rank_data_html <- html_nodes(web_page, '.text-primary')
#Converting the ranking data to text
rank_data <- html_text(rank_data_html)
#Let's have a look at the rankings
head(rank_data)
#Data-Preprocessing: Converting rankings to numerical
rank_data <- as.numeric(rank_data)
#Let's have another look at the rankings
head(rank_data)
#Using CSS selectors to scrap the title section
title_data_html <- html_nodes(webpage, '.lister-item-header a')
library(rvest)
# Specifying url of website to be scrapped
url <- 'https://www.imdb.com/search/title?release_date=2017-01-01.2017-12-31'
# Reading the HTML code from the website
web_page <- read_html(url)
# Quick look at the contents of web_page
head(web_page)
str(web_page)
rank_data_html <- html_nodes(web_page, '.text-primary')
head(rank_data_html, 10)
library(rvest)
# Specifying url of website to be scrapped
url <- 'https://www.imdb.com/search/title?release_date=2017-01-01.2017-12-31'
# Reading the HTML code from the website
web_page <- read_html(url)
# Quick look at the contents of web_page
head(web_page)
str(web_page)
# Using CSS selectors to scrap the rankings section - IMDB site is 
# examined for this information first
rank_data_html <- html_nodes(web_page, '.text-primary')
head(rank_data_html, 10)
# Converting the ranking data from HTML to text
rank_data <- html_text(rank_data_html)
# Let's have a look at the rankings data
head(rank_data, 10)
# Data-Preprocessing: Converting rankings from string to numeric
rank_data <- as.numeric(rank_data)
# Let's have another look at the rankings data
head(rank_data, 10)
# Using selected CSS tag to scrap the title section
title_data_html <- html_nodes(web_page, '.lister-item-header a')
# Converting the title section data to text
title_data <- html_text(title_data_html)
# Let's have a look at the title
head(title_data, 10)
description_data_html <- html_nodes(web_page,'.ratings-bar+ .text-muted')
# Converting the description data to text
description_data <- html_text(description_data_html)
# Let's have a look at the description data
head(description_data, 10)
# Using CSS selectors to scrap the Movie runtime section
runtime_data_html <- html_nodes(web_page, '.text-muted .runtime')
# Converting the runtime data to text
runtime_data <- html_text(runtime_data_html)
head(runtime_data, 10)
description_data <- gsub("\n", "", description_data)
head(description_data, 10)
runtime_data <- html_text(runtime_data_html)
# Let's have a look at the runtime
head(runtime_data, 10)
# "min" text inside running time values are going to be a problem
# Remove it using gsub function. And convert it using the as.numeric convertor
runtime_data <- gsub(" min", "", runtime_data)
runtime_data <- as.numeric(runtime_data)
#Let's have another look at the runtime data
head(runtime_data, 10)
# Using CSS selectors to scrape the Movie genre section
genre_data_html <- html_nodes(web_page, '.genre')
# Converting the genre data to text
genre_data <- html_text(genre_data_html)
# Let's have a look at the genre data
head(genre_data)
genre_data<-gsub("\n","",genre_data)
genre_data<-gsub(" ","",genre_data)
head(genre_data,10)
genre_data <- gsub(",.*", "", genre_data)
# Now let's examine the genre_data
head(genre_data, 10)
rating_data_html <- html_nodes(webpage, '.ratings-imdb-rating strong')
rating_data_html <- html_nodes(web_page, '.ratings-imdb-rating strong')
# Converting the ratings data to text
rating_data <- html_text(rating_data_html)
# Let's have a look at the ratings data
head(rating_data)
# Data-Preprocessing: converting ratings to numerical values
rating_data <- as.numeric(rating_data)
# Let's have another look at the ratings data
head(rating_data)
directors_data_html <- html_nodes(webpage, '.text-muted+ p a:nth-child(1)')
directors_data_html <- html_nodes(web_page, '.text-muted+ p a:nth-child(1)')
directors_data <- html_text(directors_data_html)
head(directors_data)
head(directors_data, 10)
directors_data_html <- html_nodes(web_page, '.text-muted+ p a:nth-child(1)')
#Converting the directors data to text
directors_data <- html_text(directors_data_html)
#Let's have a look at the directors data
head(directors_data, 10)
#Data-Preprocessing: converting directors data into factors
directors_data <- as.factor(directors_data)
#Using CSS selectors to scrap the actors section
actors_data_html <- html_nodes(webpage, '.lister-item-content .ghost+ a')
actors_data_html <- html_nodes(web_page, '.lister-item-content .ghost+ a')
actors_data <- html_text(actors_data_html)
head(actors_data)
metascore_data_html <- html_nodes(web_page, '.metascore')
#Converting the runtime data to text
metascore_data <- html_text(metascore_data_html)
#Let's have a look at the metascore 
data head(metascore_data)
head(metascore_data)
is.na(metascore_data)
na_count <- is.na(metascore_data)
sum(na_count)
metascore_data <- gsub(" ", "", metascore_data)
na_count <- is.na(metascore_data)
sum(na_count)
length(metascore_data)
metascore_data <- gsub(" ", "", metascore_data)
length(metascore_data)
head(metascore_data, 100)
metascore_data <- html_text(metascore_data_html)
head(metascore_data)
head(metascore_data, 100)
metascore_data_html <- html_nodes(web_page, 'ratings-metascore')
metascore_data <- html_text(metascore_data_html)
head(metascore_data, 100)
metascore_data_html <- html_nodes(web_page, '.mixed')
metascore_data <- html_text(metascore_data_html)
head(metascore_data, 100)
metascore_data_html <- html_nodes(web_page, '.metascore mixed')
#Converting the runtime data to text
metascore_data <- html_text(metascore_data_html)
#Let's have a look at the metascore 
head(metascore_data)
metascore_data_html <- html_nodes(web_page, '.metascore-mixed')
#Converting the runtime data to text
metascore_data <- html_text(metascore_data_html)
#Let's have a look at the metascore 
head(metascore_data)
metascore_data_html <- html_nodes(web_page, 'metascore-mixed')
#Converting the runtime data to text
metascore_data <- html_text(metascore_data_html)
#Let's have a look at the metascore 
head(metascore_data)
metascore_data_html <- html_nodes(web_page, '.ratings-metascore')
#Converting the runtime data to text
metascore_data <- html_text(metascore_data_html)
#Let's have a look at the metascore 
head(metascore_data)
length(metascore_data)
metascore_data <- gsub(" ", "", metascore_data)
head(metascore_data, 100)
length(metascore_data)
metascore_data <- gsub("\n", "", metascore_data)
length(metascore_data)
head(metascore_data, 100)
metascore_data <- gsub("Metascore", "", metascore_data)
head(metascore_data, 100)
metascore_data_html <- html_nodes(web_page, '.ratings-metascore')
#Converting the runtime data to text
metascore_data <- html_text(metascore_data_html)
#Let's have a look at the metascore 
head(metascore_data, 10)
na_count <- is.na(metascore_data)
sum(na_count)
summary(metascore_data)
gross_data_html <- html_nodes(web_page, '.ghost~ .text-muted+ span')
#Converting the gross revenue data to text
gross_data <- html_text(gross_data_html)
#Let's have a look at the votes data
head(gross_data)
gross_data_html <- html_nodes(web_page, '.text-muted+ span')
#Converting the gross revenue data to text
gross_data <- html_text(gross_data_html)
#Let's have a look at the votes data
head(gross_data)
gross_data_html <- html_nodes(web_page, '.ghost~ .text-muted+ span')
#Converting the gross revenue data to text
gross_data <- html_text(gross_data_html)
#Let's have a look at the votes data
head(gross_data)
gross_data_html <- html_nodes(web_page, '.ghost .text-muted+ span')
#Converting the gross revenue data to text
gross_data <- html_text(gross_data_html)
#Let's have a look at the votes data
head(gross_data)
gross_data_html <- html_nodes(web_page, '.ghost+ .text-muted+ span')
#Converting the gross revenue data to text
gross_data <- html_text(gross_data_html)
#Let's have a look at the votes data
head(gross_data)
#Data-Preprocessing: removing 'M' sign
gross_data <- gsub("M", "", gross_data)
head(gross_data, 10)
gross_data <- substring(gross_data, 2, 6)
head(gross_data, 10)
gross_data <- html_text(gross_data_html)
#Let's have a look at the votes data
head(gross_data)
#Data-Preprocessing: removing 'M' sign
gross_data <- gsub("M", "", gross_data)
head(gross_data, 10)
gross_data <- substring(gross_data, 2, 7)
#Let's check the length of gross data
length(gross_data)
head(gross_data, 10)
length(gross_data)
summary(gross_data)
head(gross_data, 50)
library(rvest)
url <- 'https://www.imdb.com/search/title?release_date=2017-01-01.2017-12-31'
# Reading the HTML code from the website
web_page <- read_html(url)
head(web_page)
str(web_page)
rank_data_html <- html_nodes(web_page, '.text-primary')
head(rank_data_html, 10)
rank_data <- html_text(rank_data_html)
head(rank_data, 10)
rank_data <- as.numeric(rank_data)
head(rank_data, 10)
title_data_html <- html_nodes(web_page, '.lister-item-header a')
title_data <- html_text(title_data_html)
head(title_data, 10)
description_data_html <- html_nodes(web_page,'.ratings-bar+ .text-muted')
description_data <- html_text(description_data_html)
head(description_data, 10)
description_data <- gsub("\n", "", description_data)
head(description_data, 10)
runtime_data_html <- html_nodes(web_page, '.text-muted .runtime')
runtime_data <- html_text(runtime_data_html)
head(runtime_data, 10)
runtime_data <- gsub(" min", "", runtime_data)
runtime_data <- as.numeric(runtime_data)
head(runtime_data, 10)
genre_data_html <- html_nodes(web_page, '.genre')
genre_data <- html_text(genre_data_html)
head(genre_data)
genre_data<-gsub("\n","",genre_data)
genre_data<-gsub(" ","",genre_data)
head(genre_data,10)
genre_data <- gsub(",.*", "", genre_data)
head(genre_data, 10)
rating_data_html <- html_nodes(web_page, '.ratings-imdb-rating strong')
rating_data <- html_text(rating_data_html)
head(rating_data)
rating_data <- as.numeric(rating_data)
head(rating_data, 10)
directors_data_html <- html_nodes(web_page, '.text-muted+ p a:nth-child(1)')
head(directors_data, 10)
directors_data <- html_text(directors_data_html)
head(directors_data, 10)
directors_data <- as.factor(directors_data)
actors_data_html <- html_nodes(web_page, '.lister-item-content .ghost+ a')
actors_data <- html_text(actors_data_html)
head(actors_data)
actors_data <- as.factor(actors_data)
metascore_data_html <- html_nodes(web_page, '.ratings-metascore')
#Converting the data to text
metascore_data <- html_text(metascore_data_html)
#Let's have a look at the metascore 
head(metascore_data, 10)
# Data-Preprocessing: removing extra space in metascore data
metascore_data <- gsub(" ", "", metascore_data)
head(metascore_data, 100)
# Data-Preprocessing: removing \n
metascore_data <- gsub("\n", "", metascore_data)
# Data pre-processing: remove "metascore" from data
metascore_data <- gsub("Metascore", "", metascore_data)
# Data should contain 50 values (50 movies)
length(metascore_data)
summary(metascore_data)
# There's lots of metascore data missing
na_count <- is.na(metascore_data)
sum(na_count)
# Scrape the gross revenue section
gross_data_html <- html_nodes(web_page, '.ghost+ .text-muted+ span')
#Converting the gross revenue data to text
gross_data <- html_text(gross_data_html)
#Let's have a look at the votes data
head(gross_data)
# Data-Preprocessing: removing 'M' sign
gross_data <- gsub("M", "", gross_data)
head(gross_data, 10)
# Data-Preprocessing: removing '$' sign with substring
gross_data <- substring(gross_data, 2, 7)
#Let's check the length of gross data
# Should be 50 - lots of data missing
length(gross_data)
movies <- data.frame(rank = rank_data,                     Title = title_data,                     Description = description_data,                     Runtime = runtime_data,                     Genre = genre_data,                     Rating = rating_data,                     Metascore = metascore_data,                     Votes = votes_data,                     Gross = gross_data,                     Director = directors_data,                     Actor = actors_data)
movies <- data.frame(rank = rank_data,                     Title = title_data,                     Description = description_data,                     Runtime = runtime_data,                     Genre = genre_data,                     Rating = rating_data,                     Metascore = metascore_data,                     Gross = gross_data,                     Director = directors_data,                     Actor = actors_data)
movies <- data.frame(Rank = rank_data,                     Title = title_data,                     Description = description_data,                     Runtime = runtime_data,                     Genre = genre_data,                     Rating = rating_data,                     Metascore = metascore_data,                     Gross = gross_data,                     Director = directors_data,                     Actor = actors_data)
movies <- data.frame(Rank = rank_data,                     Title = title_data,                     Description = description_data                     )
movies <- data.frame(Rank = rank_data, Title = title_data, Description = description_data)
movies <- data.frame(rank = rank_data, Title = title_data, Description = description_data, Runtime = runtime_data,                     Genre = genre_data,                     Rating = rating_data,                     Metascore = metascore_data,                     Gross = gross_data,                     Director = directors_data,                     Actor = actors_data)
movies
movies <- data.frame(rank = rank_data, Title = title_data, Description = description_data, )
movies <- data.frame(rank = rank_data, Title = title_data, Description = description_data )
movies <- data.frame(rank = rank_data, Title = title_data )
movies
movies <- data.frame(rank = rank_data, Title = title_data, Desc = description_data )
head(description_data)
head(description_data, 50)
length(description_data)
description_data_html <- html_nodes(web_page,'.text-muted')
description_data <- html_text(description_data_html)
head(description_data, 10)
description_data_html <- html_nodes(web_page,'.ratings-bar+ .text-muted')
description_data <- html_text(description_data_html)
head(description_data, 10)
description_data_html <- html_nodes(web_page,'.ratings-bar+ .text-muted+')
description_data_html <- html_nodes(web_page,'.ratings-bar+ .text-muted+ .text-muted')
description_data <- html_text(description_data_html)
head(description_data, 10)
description_data_html <- html_nodes(web_page,'.ratings-bar+ .text-muted+ .text-muted')
description_data <- html_text(description_data_html)
head(description_data, 10)
description_data_html <- html_nodes(web_page,'.ratings-bar+ .text-muted')
# Converting the description data to text
description_data <- html_text(description_data_html)
# Let's have a look at the description data
head(description_data, 10)
description_data_html <- html_nodes(web_page,'.text-muted')
description_data <- html_text(description_data_html)
head(description_data, 10)
description_data <- gsub("\n *", "", description_data)
head(description_data, 10)
description_data <- gsub("Votes:", "", description_data)
head(description_data, 10)
description_data <- gsub("(*)", "", description_data)
head(description_data, 10)
description_data <- gsub("(*", "", description_data)
description_data <- gsub("(*)", "", description_data)
# Let's have a look at the description data
head(description_data, 10)
description_data_html <- html_nodes(web_page, '.text-muted')
description_data <- html_text(description_data_html)
description_data <- gsub("\n *", "", description_data)
head(description_data, 10)
description_data <- gsub("[[:punct:]]", "", description_data)
head(description_data, 10)
head(description_data, 50)
description_data <- gsub("Votes", "", description_data)
head(description_data, 50)
# Tidy the decription data to remove the "\n" control character
head(description_data, 50)
description_data <- gsub("201*", "", description_data)
head(description_data, 50)
description_data_html <- html_nodes(web_page, '.text-muted')
# Converting the description data to text
description_data <- html_text(description_data_html)
# Delete all text after "\n"
description_data <- gsub("\n *", "", description_data)
# Delete all text with punctuation included
# See http://www.endmemo.com/program/R/gsub.php for details
description_data <- gsub("[[:punct:]]", "", description_data)
# Delete all text with "Votes"
description_data <- gsub("Votes", "", description_data)
# Delete all text with "Votes"
description_data <- gsub("201.*", "", description_data)
head(description_data, 50)
description_data <- gsub("Gross", "", description_data)
head(description_data, 50)
description_data <- gsub("*. min *", "", description_data)
# Let's have a look at the description data
head(description_data, 50)
description_data <- gsub("^[0-9]", "", description_data)
head(description_data, 50)
description_data <- gsub(".*[0-9]", "", description_data)
# Let's have a look at the description data
head(description_data, 50)
description_data_html <- html_nodes(web_page, '.text-muted')
# Converting the description data to text
description_data <- html_text(description_data_html)
# Delete all text after "\n"
description_data <- gsub("\n *", "", description_data)
# Delete all text with punctuation included
# See http://www.endmemo.com/program/R/gsub.php for details
description_data <- gsub("[[:punct:]]", "", description_data)
# Delete all text with "Votes"
description_data <- gsub("Votes", "", description_data)
# Delete all text with "201*"
description_data <- gsub("201.*", "", description_data)
description_data <- gsub("Gross", "", description_data)
# Delete all text with a number in it - eg PG12A
description_data <- gsub(".*[0-9]", "", description_data)
# Let's have a look at the description data
head(description_data, 50)
description_data <- gsub(". min ", "", description_data)
head(description_data, 50)
description_data <- gsub(".* min ", "", description_data)
# Let's have a look at the description data
head(description_data, 50)
summary(description_data)
is.na(description_data)
description_data <- description_data[== ""] <- NA
description_data <- description_data[description_data == ""] <- NA
head(description_data, 50)
description_data_html <- html_nodes(web_page, '.text-muted')
# Converting the description data to text
description_data <- html_text(description_data_html)
# Delete all text after "\n"
description_data <- gsub("\n *", "", description_data)
# Delete all text with punctuation included
# See http://www.endmemo.com/program/R/gsub.php for details
description_data <- gsub("[[:punct:]]", "", description_data)
# Delete all text with "Votes"
description_data <- gsub("Votes", "", description_data)
# Delete all text with "201*"
description_data <- gsub("201.*", "", description_data)
# Delete all text with "Gross"
description_data <- gsub("Gross", "", description_data)
# Delete all text with a number in it - eg PG12A
description_data <- gsub(".*[0-9]", "", description_data)
description_data <- gsub(".* min ", "", description_data)
head(description_data, 50)
description_data <- description_data[description_data == " "] <- NA
head(description_data, 50)
description_data_html <- html_nodes(web_page, '.text-muted')
# Converting the description data to text
description_data <- html_text(description_data_html)
# Delete all text after "\n"
description_data <- gsub("\n *", "", description_data)
# Delete all text with punctuation included
# See http://www.endmemo.com/program/R/gsub.php for details
description_data <- gsub("[[:punct:]]", "", description_data)
# Delete all text with "Votes"
description_data <- gsub("Votes", "", description_data)
# Delete all text with "201*"
description_data <- gsub("201.*", "", description_data)
# Delete all text with "Gross"
description_data <- gsub("Gross", "", description_data)
# Delete all text with a number in it - eg PG12A
description_data <- gsub(".*[0-9]", "", description_data)
description_data <- gsub(".* min ", "", description_data)
head(description_data, 10)
str(description_data)
summary(description_data)
class(description_data)
for (item in description_data)     {     if (length(item)>20) {description_data <- description_data + item     } }
head(description_data, 10)
for (item in description_data)     {     if (length(item)>50) {description_data <- description_data + item     } }
head(description_data, 10)
for (item in description_data)     {     if (nchar(item)>50) {description_data <- description_data + item     } }
text_test <- lapply(description_data, check_length)
check_length <- function(character_to_check)     {     char_ok = NULL     if (length(character_to_check > 50))         {         return (character_to_check)     } }
text_test <- lapply(description_data, check_length)
head(text_test)
head(text_test, 50)
text_test <- lapply(description_data, check_length, na.rm = TRUE)
text_test <- lapply(description_data, check_length, na.rm = TRUE)
class(text_test)
text_test <- apply(description_data, check_length)
get_reviews <- function(html) {     html %>%     # The relevant tag     html_nodes('.text-muted') %>%         html_text() %>%     # Trim additional white space     str_trim() %>%     # Convert the list into a vector     unlist() }
description_data <- get_reviews(web_page)
library(tidyverse)
library(rvest)
library(stringr)
description_data <- get_reviews(web_page)
head(description_data)
desc_data = description_data[,3]
desc_data = description_data[3,]
description_data <- get_reviews(web_page)
head(description_data)
new_data <- description_data[c(3)]
new_data
description_data <- get_reviews(web_page)
url <- 'https://www.imdb.com/search/title?release_date=2017-01-01.2017-12-31'
# Reading the HTML code from the website
web_page <- read_html(url)
description_data <- get_reviews(web_page)
head(description_data, 10)
get_reviews <- function(html) {     html %>%     # The relevant tag      html_nodes('.ratings-bar+ .text-muted') %>%     html_text() %>%     # Trim additional white space      str_trim() %>%     # Convert the list into a vector      unlist() }
description_data <- get_reviews(web_page)
head(description_data, 10)
get_reviews <- function(html) {     html %>%     # The relevant tag      html_nodes('.text-muted') %>%     html_text() %>%     # Trim additional white space      str_trim() %>%     # Convert the list into a vector      unlist() }
description_data <- get_reviews(web_page)
head(description_data, 10)
?html_text
description_data_html <- html_nodes(web_page, '.text-muted')
head(description_data, 10)
description_data_html <- html_nodes(web_page, 'text-muted')
head(description_data, 10)
description_data_html <- html_nodes(web_page, 'text-muted')
description_data <- html_text(description_data_html)
head(description_data, 10)
description_data_html <- html_nodes(web_page, '#text-muted')
description_data <- html_text(description_data_html)
#description_data <- get_reviews(web_page)
head(description_data, 10)
description_data_html <- html_nodes(web_page, '.text-muted')
description_data <- html_text(description_data_html)
head(description_data, 10)
url <- 'https://www.imdb.com/search/title?release_date=2017-01-01.2017-12-31'
web_page <- read_html(url)
head(web_page)
url <- 'https://www.imdb.com/search/title?release_date=2017-01-01.2017-12-31'
# Reading the HTML code from the website
web_page <- read_html(url)
library(rvest)
# Specifying url of website to be scrapped
url <- 'https://www.imdb.com/search/title?release_date=2017-01-01.2017-12-31'
# Reading the HTML code from the website
web_page <- read_html(url)
